{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements/Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CSV files (tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = os.path.join('.', 'WikidataTables2024R1', 'DataSets', 'Valid', 'tables')\n",
    "table_paths = os.listdir(dir)\n",
    "table_paths = [os.path.abspath(os.path.join(dir, p)) for p in table_paths]\n",
    "tables = {}\n",
    "for table_path in table_paths:\n",
    "    table_name, _ = os.path.splitext(os.path.basename(table_path))\n",
    "    df = pd.read_csv(table_path, sep=',', quotechar='\"',escapechar=\"\\\\\")\n",
    "    tables[table_name] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Querying functions for wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_wikidata(query):\n",
    "    sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    results = sparql.query().convert()\n",
    "    return results\n",
    "\n",
    "\n",
    "def get_wikidata_entity(label):\n",
    "    query = f\"\"\"\n",
    "    SELECT ?entity ?entityLabel WHERE {{\n",
    "      ?entity ?label \"{label}\"@en.\n",
    "      FILTER STRSTARTS(STR(?entity), \"http://www.wikidata.org/entity/\")\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }} LIMIT 10\n",
    "    \"\"\"\n",
    "    results = query_wikidata(query)\n",
    "    return results['results']['bindings']\n",
    "\n",
    "\n",
    "def get_column_type(column_values):\n",
    "    # Convert column values to strings and ensure proper escaping of quotes\n",
    "    column_values = [str(value).replace('\"', '\\\\\"') for value in column_values]\n",
    "    values_str = ' '.join([f'\"{value}\"@en' for value in column_values])\n",
    "    \n",
    "    # Construct the SPARQL query\n",
    "    query = f\"\"\"\n",
    "    SELECT ?type ?typeLabel WHERE {{\n",
    "      VALUES ?value {{ {values_str} }}\n",
    "      ?entity rdfs:label ?value.\n",
    "      ?entity wdt:P31 ?type.\n",
    "      SERVICE wikibase:label {{ bd:serviceParam wikibase:language \"en\". }}\n",
    "    }} LIMIT 100\n",
    "    \"\"\"\n",
    "    \n",
    "    results = query_wikidata(query)\n",
    "    return results['results']['bindings']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple disambiguation techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disambiguate_entities(entities):\n",
    "    entity_labels = [entity['entity']['value'] for entity in entities]\n",
    "    most_common_entity = Counter(entity_labels).most_common(1)[0][0]\n",
    "    return most_common_entity\n",
    "\n",
    "def disambiguate_column_type(types):\n",
    "    type_labels = [t['type']['value'] for t in types]\n",
    "    most_common_type = Counter(type_labels).most_common(1)[0][0]\n",
    "    return most_common_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matching functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match cells to entities\n",
    "def match_cells_to_entities(df):\n",
    "    matched_entities = []\n",
    "    for row_id, row in df.iterrows():\n",
    "        for column_id, cell in enumerate(row):\n",
    "            if pd.notna(cell):\n",
    "                entities = get_wikidata_entity(cell)\n",
    "                if entities:\n",
    "                    matched_entity = disambiguate_entities(entities)\n",
    "                    matched_entities.append((row_id, column_id, matched_entity))\n",
    "    return matched_entities\n",
    "\n",
    "\n",
    "# Match columns to types\n",
    "def match_columns_to_types(df):\n",
    "    matched_types = {}\n",
    "    for column_id, column in enumerate(df.columns):\n",
    "        column_values = df[column].dropna().unique()\n",
    "        types = get_column_type(column_values)\n",
    "        if types:\n",
    "            matched_type = disambiguate_column_type(types)\n",
    "            matched_types[column_id] = matched_type\n",
    "    return matched_types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process multiple CSV files and save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [22:43<00:00,  2.73s/it]\n"
     ]
    }
   ],
   "source": [
    "results_cea, results_cta = [], []\n",
    "\n",
    "for table_name, df in tqdm(tables.items()):\n",
    "    try:\n",
    "        matched_entities = match_cells_to_entities(df)\n",
    "        matched_types = match_columns_to_types(df)\n",
    "    except:\n",
    "        # print(f\"Error in table {table_name}\")\n",
    "        continue\n",
    "    for row_id, column_id, entity in matched_entities:\n",
    "        results_cea.append([table_name, row_id, column_id, entity])\n",
    "    for column_id, entity in matched_types.items():\n",
    "        results_cta.append([table_name, column_id, entity])\n",
    "\n",
    "# transform results to a dataframe\n",
    "cea_df = pd.DataFrame(results_cea, columns=['Column1', 'Column2', 'Column3', 'Column4'])\n",
    "cta_df = pd.DataFrame(results_cta, columns=['Column1', 'Column2', 'Column3'])\n",
    "\n",
    "# save the results to the targets directory\n",
    "dir = os.path.join('.', 'WikidataTables2024R1', 'DataSets', 'Valid')\n",
    "cea_df.to_csv(os.path.join(dir, 'targets', 'cea_targets.csv'), index=False)\n",
    "cta_df.to_csv(os.path.join(dir, 'targets', 'cta_targets.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.047 0.060 0.038\n",
      "CEA results: \n",
      "{'score': 0.04690647482014388, 'score_secondary': 0.060303366629670736}\n",
      "0.607 0.671 0.554\n",
      "CTA results: \n",
      "{'score': 0.6068601583113457, 'score_secondary': 0.6712062256809338}\n"
     ]
    }
   ],
   "source": [
    "from CEA_Evaluator import CEA_Evaluator\n",
    "from CTA_Evaluator import CTA_Evaluator\n",
    "\n",
    "\n",
    "gt_file = os.path.join(dir, 'gt', 'cea_gt.csv')\n",
    "system_file = os.path.join(dir, 'targets', 'cea_targets.csv')\n",
    "\n",
    "# Instantiate an evaluator\n",
    "evaluator = CEA_Evaluator()\n",
    "# Evaluate\n",
    "result = evaluator._evaluate(system_file, gt_file)\n",
    "print(\"CEA results: \")\n",
    "print(result)\n",
    "\n",
    "gt_file = os.path.join(dir, 'gt', 'cta_gt.csv')\n",
    "system_file = os.path.join(dir, 'targets', 'cta_targets.csv')\n",
    "\n",
    "# Instantiate an evaluator\n",
    "evaluator = CTA_Evaluator()\n",
    "# Evaluate\n",
    "result = evaluator._evaluate(system_file, gt_file)\n",
    "print(\"CTA results: \")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_matched_results(matched_entities, matched_types):\n",
    "#     print(\"Matched Entities:\")\n",
    "#     for cell, entity in matched_entities.items():\n",
    "#         print(f\"{cell} -> {entity}\")\n",
    "    \n",
    "#     print(\"\\nMatched Column Types:\")\n",
    "#     for column, column_type in matched_types.items():\n",
    "#         print(f\"{column} -> {column_type}\")\n",
    "\n",
    "# print_matched_results(matched_entities, matched_types)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
